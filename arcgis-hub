#!/usr/bin/env python

import argparse
import datetime
import functools
import itertools
import pathlib
import shutil
import sys
import time
from typing import Optional, Sequence

import requests
import tabulate
import werkzeug.http

BASE_URL = "https://opendata.arcgis.com/api/v3"


def print_additional_records(meta, limit):
    total = meta["stats"]["totalCount"]
    if args.limit and total > limit:
        print(
            f"{total - limit} more results available; set `--limit` for more",
            file=sys.stderr,
        )


def iter_results(response):
    data = response.json()
    for result in data["data"]:
        yield result
    if data["meta"].get("next") is not None:
        response = requests.get(data["meta"]["next"])
        response.raise_for_status()
        yield from iter_results(response)


def get_dataset(dataset_id: str):
    response = requests.get(f"{BASE_URL}/datasets/{dataset_id}")
    response.raise_for_status()
    return response.json()["data"]


def list_datasets(
    group_ids: Optional[Sequence[str]] = None,
    tags: Optional[Sequence[str]] = None,
    query: str = "*",
):
    filter_ = {
        "openData": True,
        "collection": "any(Dataset)",
    }
    if tags:
        filter_["tags"] = f"any({','.join(tags)})"
    catalog = {}
    if group_ids:
        catalog["groupIds"] = f"any({','.join(group_ids)})"
    response = requests.post(
        f"{BASE_URL}/search",
        json={
            "q": query,
            "filter": filter_,
            "catalog": catalog,
        },
    )
    response.raise_for_status()
    return response.json()["meta"], iter_results(response)


def get_or_trigger_download(
    dataset_id: str,
    path: pathlib.Path,
    stale_timestamp: datetime.datetime,
    format_: str = "shp",
):
    download = get_download(dataset_id, format_)

    # Handle datasets that don't exist, like
    # https://open-data.bouldercolorado.gov/datasets/17d03a45de814e62b8d7b3d603a61448_0/about
    if download is None:
        print("\tDownload not found; skipping")
        return

    # Trigger a new download if the dataset allows and the latest download is stale
    if can_trigger_download(download) and is_download_stale(download, stale_timestamp):
        format_string = "%Y-%m-%d %H:%M:%S"
        last_modified = get_download_last_modified(download)
        print(
            f"\tExisting download created at {last_modified.strftime(format_string)}, "
            f"must be newer than {stale_timestamp.strftime(format_string)}; "
            "triggering new download"
        )
        trigger_download(dataset_id, format_, download["attributes"]["spatialRefId"])

    return download


def poll_downloads(
    downloads,
    path: pathlib.Path,
    stale_timestamp: datetime.datetime,
    format_: str = "shp",
):
    failed = {}
    path.mkdir(parents=True, exist_ok=True)
    while True:
        next_downloads = {}
        for dataset_id, data in downloads.items():
            dataset = data["dataset"]
            download = data["download"]
            dataset_name = dataset["attributes"]["name"]
            status = download["attributes"]["status"]
            if did_download_error(download):
                print(
                    f'\tDownload for {dataset_name} failed with status "{status}"; skipping'
                )
                failed[dataset_id] = data
            elif should_poll_download(download, stale_timestamp):
                print(f'\tDownload status for {dataset_name} is "{status}"; retrying')
                next_downloads[dataset_id] = {
                    "dataset": dataset,
                    "download": get_download(dataset_id, format_),
                }
            else:
                download_file(
                    download["links"]["content"], path.joinpath(f"{dataset_name}.zip")
                )
        if next_downloads:
            print("Waiting...")
            time.sleep(15)
            downloads = next_downloads
        else:
            break
    return failed


def can_trigger_download(payload) -> bool:
    """Downloads marked "ready" don't appear to be able to generate new exports."""
    attributes = payload["attributes"]
    return attributes["status"] != "ready"


def should_poll_download(payload, stale_timestamp: datetime.datetime) -> bool:
    # Don't poll download if the export is static
    if not can_trigger_download(payload):
        return False
    return is_download_stale(payload, stale_timestamp) or not is_download_ready(payload)


def is_download_ready(payload) -> bool:
    attributes = payload["attributes"]
    return attributes["status"] in {"ready", "ready_unknown"}


def did_download_error(payload) -> bool:
    attributes = payload["attributes"]
    return attributes["status"] in {"error_updating"}


def is_download_stale(payload, stale_timestamp: datetime.datetime) -> bool:
    last_modified = get_download_last_modified(payload)
    return last_modified < stale_timestamp


def get_download_last_modified(payload):
    attributes = payload["attributes"]
    return datetime.datetime.strptime(
        attributes["lastModified"], "%Y-%m-%dT%H:%M:%S.%f%z"
    )


def download_file(url: str, path: pathlib.Path):
    with requests.get(url, stream=True) as response:
        response.raise_for_status()
        # Decompressed gzipped responses
        # https://github.com/psf/requests/issues/2155#issuecomment-50771010
        response.raw.read = functools.partial(response.raw.read, decode_content=True)
        with path.open("wb") as fp:
            shutil.copyfileobj(response.raw, fp)


def get_download(dataset_id: str, format_: str = "shp"):
    response = requests.get(
        f"{BASE_URL}/datasets/{dataset_id}/downloads",
        params={"formats": format_},
    )
    response.raise_for_status()
    data = response.json()
    return data["data"][0] if data["data"] else None


def trigger_download(dataset_id: str, format_: str, spatial_ref_id: str):
    response = requests.post(
        f"{BASE_URL}/datasets/{dataset_id}/downloads",
        json={"format": format_, "spatialRefId": spatial_ref_id},
    )
    response.raise_for_status()


def search_sites(args):
    response = requests.post(
        f"{BASE_URL}/search",
        json={
            "q": args.query,
            "filter": {
                "openData": True,
                "collection": "any(Site)",
            },
        },
    )
    response.raise_for_status()
    datasets = itertools.islice(iter_results(response), args.limit)
    results = [
        [
            dataset["attributes"].get("name"),
            dataset["attributes"].get("url"),
            ", ".join(get_site_data(dataset, "groups") or []),
        ]
        for dataset in datasets
    ]
    print(tabulate.tabulate(results, headers=["Name", "URL", "Group IDs"]))
    print_additional_records(response.json()["meta"], args.limit)


def get_site_data(site, key):
    response = requests.get(site["links"]["rawEs"])
    response.raise_for_status()
    data = response.json()
    try:
        return data["_source"]["data"]["catalog"].get(key)
    except KeyError:
        return None


def search_datasets(args):
    meta, it = list_datasets(args.group_id, args.tag, args.query)
    datasets = itertools.islice(it, args.limit)
    results = [
        [
            dataset["attributes"].get("name"),
            dataset["id"],
            ", ".join(dataset["attributes"].get("tags", [])),
            dataset["attributes"].get("url"),
        ]
        for dataset in datasets
    ]
    print(tabulate.tabulate(results, headers=["Name", "ID", "Tags", "URL"]))
    print_additional_records(meta, args.limit)


def fetch_datasets(args):
    now = datetime.datetime.now(datetime.timezone.utc)
    stale_timestamp = now - datetime.timedelta(hours=args.stale_interval_hours)
    meta, datasets = list_datasets(args.group_id, args.tag, args.query)
    if args.limit:
        datasets = itertools.islice(datasets, args.limit)
    downloads = {}
    for dataset in datasets:
        print(f"Fetching dataset {dataset['attributes']['name']}")
        download = get_or_trigger_download(dataset["id"], args.path, stale_timestamp)
        if download:
            downloads[dataset["id"]] = {"download": download, "dataset": dataset}
    failed = poll_downloads(downloads, args.path, stale_timestamp)
    print_additional_records(meta, args.limit)
    if failed:
        failed_label = ", ".join(
            data["dataset"]["attributes"]["name"] for data in failed.values()
        )
        print(f"Failed downloads: {failed_label}")
        sys.exit(1)


def fetch_datasets_by_id(args):
    now = datetime.datetime.now(datetime.timezone.utc)
    stale_timestamp = now - datetime.timedelta(hours=args.stale_interval_hours)
    downloads = {}
    for dataset_id in args.dataset_id:
        print(f"Fetching dataset {dataset_id}")
        dataset = get_dataset(dataset_id)
        download = get_or_trigger_download(dataset_id, args.path, stale_timestamp)
        if download:
            downloads[dataset_id] = {"download": download, "dataset": dataset}
    failed = poll_downloads(downloads, args.path, stale_timestamp)
    if failed:
        failed_label = ", ".join(
            data["dataset"]["attributes"]["name"] for data in failed.values()
        )
        print(f"Failed downloads: {failed_label}")
        sys.exit(1)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Search and download resources from ArcGIS Hub"
    )
    subparsers = parser.add_subparsers(dest="command")
    subparsers.required = True

    search_sites_parser = subparsers.add_parser(
        "search-sites", description="Search ArcGIS Hub sites"
    )
    search_sites_parser.add_argument("--query", default="*")
    search_sites_parser.add_argument("--limit", type=int, default=20)
    search_sites_parser.set_defaults(func=search_sites)

    search_datasets_parser = subparsers.add_parser(
        "search-datasets", description="Search ArcGIS datasets"
    )
    search_datasets_parser.add_argument("--query", default="*")
    search_datasets_parser.add_argument("--group-id", nargs="+")
    search_datasets_parser.add_argument("--tag", nargs="+")
    search_datasets_parser.add_argument("--limit", type=int, default=20)
    search_datasets_parser.set_defaults(func=search_datasets)

    fetch_datasets_parser = subparsers.add_parser(
        "fetch-datasets", description="Download ArcGIS datasets"
    )
    fetch_datasets_parser.add_argument("--query", default="*")
    fetch_datasets_parser.add_argument("--group-id", nargs="+")
    fetch_datasets_parser.add_argument("--tag", nargs="+")
    fetch_datasets_parser.add_argument("--limit", type=int, default=0)
    fetch_datasets_parser.add_argument(
        "--path", type=pathlib.Path, default=pathlib.Path(".")
    )
    fetch_datasets_parser.add_argument(
        "--stale-interval-hours", type=int, default=24 * 7
    )
    fetch_datasets_parser.set_defaults(func=fetch_datasets)

    fetch_datasets_by_id_parser = subparsers.add_parser(
        "fetch-datasets-by-id", description="Download ArcGIS datasets by ID"
    )
    fetch_datasets_by_id_parser.add_argument("--dataset-id", required=True, nargs="+")
    fetch_datasets_by_id_parser.add_argument(
        "--path", type=pathlib.Path, default=pathlib.Path(".")
    )
    fetch_datasets_by_id_parser.add_argument(
        "--stale-interval-hours", type=int, default=24 * 7
    )
    fetch_datasets_by_id_parser.set_defaults(func=fetch_datasets_by_id)

    args = parser.parse_args()
    args.func(args)
